[["1. Parse the XML data"], ["1. Fetch the HTML content of the page."], ["1. Get the video id (vid) from the url."], ["- Take a string and a list of color codes as arguments"], ["- Print a text with a color"], ["- print a message in yellow and bold"], ["- print a message in red and bold"], ["1. Detect the operating system of the machine where the script is running."], ["1. Get the channel id from the url"], ["1. Parse the XML string into an ElementTree object."], ["1. Generate a random number and use it to create a path"], ["1. Get the m3u8 url from the mgtv page."], ["1. Remove any characters that are not valid in a filename on the target filesystem."], ["1. Get the video's pid (a unique identifier) from the given URL."], ["1. Determine which stream to download based on the arguments passed in."], ["1. Get the sourceType and sourceId from the main parsing API."], ["1. Define a function matchall that takes two arguments: a string, text, and a list of strings, patterns."], ["1. Parse the URL to extract the query parameters."], ["1. Send a GET request to the specified URL."], ["1. Send a POST request to the specified URL with the provided headers and data."], ["1. Check if the host is a number. If it is, it's assumed to be a port number and the hostname is set to 0.0.0.0."], ["1. Get the room_id from the room_url_key."], ["- get the title of a part of a topic in a json file"], ["1. Get the course information from the json_api_content"], ["1. Get the title of the video from the JSON content"], ["- Check if the task is already in the queued_tasks or running dictionary."], ["1. Return the events in the event buffer that are associated with the given dag_ids."], ["1. Get the connection parameters from the configuration file."], ["1. Get the connection object from the Airflow connection object."], ["1. Get the field name from the user."], ["1. Check if the file exists. If not, create it."], ["1. Connect to the database"], ["1. Connect to the Google Cloud Storage"], ["1. Connect to the metastore database."], ["1. Connect to the MySQL database using the connection id provided by the Airflow connection."], ["1. Get the DAG object"], ["1. Ensure that the number of workers running matches the expected number."], ["1. Check if the _client attribute is already defined."], ["1. Connect to the Google Cloud Translation API"], ["1. Get the instance details from the Google Cloud Platform (GCP)"], ["1. Create a new instance in the Google Compute Engine."], ["1. Patch an instance with a given body."], ["1. Delete a Google Compute Engine instance."], ["1. Get a connection to the Google Cloud SQL instance."], ["1. Create a new database in the specified instance."], ["1. Patch a database in Google Cloud SQL."], ["1. Delete a database from a Google Cloud SQL instance."], ["1. Export an instance of a BigQuery dataset."], ["1. Download the sql proxy if it's not already downloaded."], ["1. Kill the process that is running the cloud_sql_proxy."], ["1. Download the SQL Proxy if it's not already downloaded."], ["1. Create a connection object"], ["1. Retrieve the connection from the Airflow database."], ["1. Query the database for connections with the specified connection id."], ["1. Check if the use_proxy attribute is True. If not, it raises an AirflowException."], ["1. Check the type of database (Postgres or MySQL)"], ["1. Check if the database type is postgres"], ["1. Create a socket object"], ["1. Add a prefix to job_id if it starts with a digit or a template."], ["1. Match the error code in the exception message."], ["1. Query the DagRun table in the Airflow database for all DAG runs that are associated with the DAGs specified in the DAG_IDS list."], ["1. Query the TaskInstance table in the database for all instances of tasks that are part of the DAGs specified in DAG_IDS."], ["1. Query the DAGs in the database."], ["1. Query the TaskInstance table in the Airflow database for all task instances that are part of the DAGs specified in DAG_IDS."], ["1. Get all the task instances that are successful."], ["1. Connect to AWS Lambda"], [""], ["1. Set the umask to 0 (which means that the permissions of the created directories will be the same as the permissions of the parent directory)"], ["- Try to convert a string to a float."], ["1. Check if the datetime is aware (i.e., it has a timezone). If it is, raise an error."], ["1. Check if the input datetime is naive (i.e., it doesn't have timezone information). If it is, the function raises a ValueError."], ["1. Accept any arguments that the datetime function in the datetime module would accept."], ["1. Get the connection details from the Airflow connection"], ["1. Create a session object"], ["1. Create a request object based on the method type (GET, POST, PUT, DELETE, etc.)"], ["1. Check if the response from the API request is successful."], ["1. Send the request using the provided session."], ["1. Create a new session."], ["1. Wrap the function that is passed in as an argument."], ["1. Drop all existing tables in the database."], ["1. Check if the exception has a 'message' attribute."], ["1. Call the super() function to call the get_records() function from the parent class."], ["1. Execute the HiveQL query and fetch the results."], ["1. Connect to the database"], ["1. Insert rows into a table."], ["1. Check if the cosmos_client attribute is already initialized."], ["1. Check if a collection with the given name exists in the database."], ["1. Check if the collection already exists."], ["1. Check if the database name is None. If it is, raise an AirflowBadRequest."], ["1. Check if the database already exists."], ["1. Check if the database_name is None. If it is, raise an AirflowBadRequest error."], ["1. Check if the collection_name is None. If it is, raise an AirflowBadRequest error."], ["1. Check if the documents are not empty"], ["1. Delete a document from the database."], ["1. Check if the document_id is None, if so raise an error."], ["1. Connect to the CosmosDB database"], ["1. Get the function by name."], ["1. Create a new function in Google Cloud Functions."], ["1. Update a Cloud Function"], ["1. Generate an upload URL for a function."], ["1. Delete a function from Google Cloud Functions."], ["1. Check if the task is ignoreable and if the context specifies that all dependencies should be ignored."], ["1. Get the status of all dependencies of a task instance (ti)"], ["1. Get the status of all dependencies of a task instance (ti)"], ["1. Parse the configuration file for AWS S3."], ["1. Get the credentials from the AWS session."], ["1. Get the connection details from the Airflow connection object."], ["1. Check if there are any messages in the buffer."], ["1. Check if the fileloc is a zip file."], ["1. Walk through the directory and its subdirectories."], ["1. Construct a TaskInstance object from the given parameters."], ["1. Launch a new process that will run the DAGs."], ["1. Send a termination message to the manager."], ["1. Log a message indicating that the program is exiting gracefully."], ["1. Log the start of the process"], ["1. Check for signals from the main process. If it receives a signal to terminate, it will break the loop and terminate the process."], ["1. Receive a signal from the agent."], ["1. Check if enough time has passed since the last refresh."], ["1. Check if enough time has passed since the last time the stats were printed."], ["1. Query the database for all ImportError instances."], ["1. Get the last runtime of each file."], ["1. Update the list of file paths that the FileWatcher is watching."], ["1. Iterate over all the processors in the _processors dictionary."], ["1"], ["1. Get all the PIDs of the processes that are running."], ["1. Create a SSH client"], ["1. Inject the project_id into the body of the request."], ["1. Get the transfer job details by job name."], ["1. Connect to the Google Cloud Storage Transfer API."], ["1. Inject the project_id into the body of the request."], ["1. Connect to the Google Cloud Storage Transfer API."], ["1. Cancel a transfer operation."], ["Pause a transfer operation."], ["- Resume a transfer operation that was previously paused."], ["1. List all the transfer operations related to a specific job."], ["1. Find all TaskReschedule records that have the same dag_id, task_id, execution_date and try_number as the given task_instance."], ["1. Query the number of running and queued tasks in the pool."], ["1. Split the command into a list of arguments"], ["1. Remove an option from the current configuration file."], ["1. Check if the section exists in the configuration file or in the airflow_defaults."], ["1. Connect to the Google Cloud Datastore."], ["1. Get a connection to the Google Cloud Storage API."], ["1. Get a connection to the Google Cloud Storage API."], ["1. Connect to the database"], ["1. Get a connection to the Google Cloud Storage API."], ["1. Connect to the Google Analytics API"], ["1. Get a connection to the Google Cloud Storage API."], ["1. Connect to the Google Cloud Storage API."], ["1. Poll the operation status until it is done."], ["1. Connect to the Google Cloud Storage bucket."], ["1. Connect to the Google Cloud Storage bucket."], ["1. Connect to the AWS SNS service."], ["1. Fetch the hostname_callable from the airflow configuration."], ["1. Create a connection to the Google Cloud Natural Language API."], ["1. Get a connection to the Google Cloud Natural Language API."], ["1. Get a connection to the Natural Language API"], ["1. Connect to the Natural Language API"], ["1. Import a module from a string."], ["1. Get the template field from the environment."], ["1. Remove the current session (which is akin to closing a connection to the database)."], ["1. Add the DAGs folder to the Python path."], ["1. Get the task instance (ti) from the context."], ["1. Get the path to the Kerberos ticket cache file from the configuration."], ["1. Check if the object is None. If it is, return None."], ["1. Take a list of items and a chunk size"], ["1. Split the iterable into chunks of a specified size."], ["1. Take a list of tasks (which are instances of the Task class) as input."], ["- Print a table with the data from the rows list."], ["1. Parse the filename_template string to determine if it is a Jinja2 template or a simple string format."], ["1. Authorize the user"], ["1. Create a DataProcOperation object with the connection and operation details."], ["1. Check if the content is a string, integer, float, list, tuple or dictionary."], ["1. Submit a run to Databricks."], ["1. Create a temporary directory and a temporary file in that directory."], ["1. Fetch the state of a celery task."], ["1. Calculate the number of tasks that can be sent in parallel."], ["1. Calculate the number of tasks that can be processed in parallel."], ["1. Check if a variable with the given key exists in the database."], ["1. Authorize the user"], ["1. Create a job in the MLEngine."], ["1. Get the job details from the MLEngine API."], ["1. Get the job status"], ["1. Create a new version of a model."], ["1. Set a specific version of a model as the default version."], ["1. List all versions of a specific model in a project."], ["1. Delete a version of a model."], ["1. Check if the model name is provided and not an empty string"], ["1. Check if the model name is provided and it is not an empty string."], ["1. Connect to the DynamoDB table."], ["1. Import all the plugins in the airflow.plugins_manager.executors_modules"], ["1. Get the default executor from the configuration."], ["1. Check if the executor_name is one of the predefined executors."], ["1. Log the error message"], ["1. Connect to the MSSQL database using the connection details provided in the Airflow connection."], ["1. Parse the JSON request data to extract the necessary parameters (run_id, conf, execution_date)"], ["1. Delete a DAG from the Airflow database."], ["1. Get the task information from the Airflow database."], ["1. Get all the pools from the pool_api"], ["1. Get the parameters from the request"], ["1. Delete a pool with a given name."]]