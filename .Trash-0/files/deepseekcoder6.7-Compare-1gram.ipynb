{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3947b06b-1bff-4546-b01f-95576b992bee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-21T02:22:03.752891Z",
     "iopub.status.busy": "2024-08-21T02:22:03.752247Z",
     "iopub.status.idle": "2024-08-21T02:22:06.940309Z",
     "shell.execute_reply": "2024-08-21T02:22:06.939610Z",
     "shell.execute_reply.started": "2024-08-21T02:22:03.752855Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: evaluate==0.3.0 in /usr/local/lib/python3.11/dist-packages (0.3.0)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate==0.3.0) (2.14.5)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate==0.3.0) (1.26.3)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate==0.3.0) (0.3.7)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate==0.3.0) (2.2.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate==0.3.0) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate==0.3.0) (4.66.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate==0.3.0) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate==0.3.0) (0.70.15)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate==0.3.0) (2023.6.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate==0.3.0) (0.20.3)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate==0.3.0) (23.2)\n",
      "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.11/dist-packages (from evaluate==0.3.0) (0.18.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate==0.3.0) (15.0.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate==0.3.0) (3.9.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from datasets>=2.0.0->evaluate==0.3.0) (5.4.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate==0.3.0) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate==0.3.0) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate==0.3.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.19.0->evaluate==0.3.0) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate==0.3.0) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.19.0->evaluate==0.3.0) (2020.6.20)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate==0.3.0) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas->evaluate==0.3.0) (2022.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate==0.3.0) (2023.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate==0.3.0) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate==0.3.0) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate==0.3.0) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate==0.3.0) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate==0.3.0) (1.3.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate==0.3.0) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install evaluate==0.3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcf78c75-6c69-4c0d-906e-d0d5da7ed7fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-21T02:21:31.880897Z",
     "iopub.status.busy": "2024-08-21T02:21:31.880362Z",
     "iopub.status.idle": "2024-08-21T02:21:31.887366Z",
     "shell.execute_reply": "2024-08-21T02:21:31.886777Z",
     "shell.execute_reply.started": "2024-08-21T02:21:31.880871Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deepseekcoder6.7/generations_deepseek-coder-6-7b-instruct\n",
      "deepseekcoder6.7/evaluation_compareeval_deepseek-coder-6-7b-instruct\n"
     ]
    }
   ],
   "source": [
    "MODEL_HUB = \"deepseek-ai/deepseek-coder-6.7b-instruct\"\n",
    "MODEL = \"deepseek-coder-6-7b-instruct\"\n",
    "PATH = \"deepseekcoder6.7\"\n",
    "TASK = \"compareeval\"\n",
    "QUANT = \"\"\n",
    "\n",
    "def create_path(model, path, task, quant):\n",
    "    SAVE_PATH = path + \"/generations_\" + model + quant\n",
    "    LOAD_PATH = SAVE_PATH + \"_\" + task + \".json\"\n",
    "    EVAL_PATH = PATH + \"/evaluation_\" + TASK + \"_\" + model + quant\n",
    "    return SAVE_PATH, LOAD_PATH, EVAL_PATH\n",
    "\n",
    "SAVE_PATH, _, EVAL_PATH = create_path(MODEL, PATH, TASK, QUANT)\n",
    "print(SAVE_PATH)\n",
    "print(EVAL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4afc6e7-49fd-49f2-95b3-65289342ea4f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-21T02:21:35.893490Z",
     "iopub.status.busy": "2024-08-21T02:21:35.893252Z",
     "iopub.status.idle": "2024-08-21T02:21:55.818522Z",
     "shell.execute_reply": "2024-08-21T02:21:55.817968Z",
     "shell.execute_reply.started": "2024-08-21T02:21:35.893473Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-21 02:21:38.401428: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-21 02:21:38.401484: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-21 02:21:38.402641: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-08-21 02:21:38.408515: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-21 02:21:39.243033: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Selected Tasks: ['compareeval']\n",
      "Loading model in bf16\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/notebooks/main.py\", line 414, in <module>\n",
      "    main()\n",
      "  File \"/notebooks/main.py\", line 296, in main\n",
      "    model = AutoModelForCausalLM.from_pretrained(\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\", line 566, in from_pretrained\n",
      "    return model_class.from_pretrained(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 3236, in from_pretrained\n",
      "    model = cls(config, *model_args, **model_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/llama/modeling_llama.py\", line 961, in __init__\n",
      "    self.model = LlamaModel(config)\n",
      "                 ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/llama/modeling_llama.py\", line 823, in __init__\n",
      "    self.layers = nn.ModuleList([LlamaDecoderLayer(config) for _ in range(config.num_hidden_layers)])\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/llama/modeling_llama.py\", line 823, in <listcomp>\n",
      "    self.layers = nn.ModuleList([LlamaDecoderLayer(config) for _ in range(config.num_hidden_layers)])\n",
      "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/llama/modeling_llama.py\", line 630, in __init__\n",
      "    LlamaAttention(config=config)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/llama/modeling_llama.py\", line 296, in __init__\n",
      "    self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py\", line 101, in __init__\n",
      "    self.reset_parameters()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py\", line 107, in reset_parameters\n",
      "    init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/init.py\", line 419, in kaiming_uniform_\n",
      "    return tensor.uniform_(-bound, bound)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python main.py \\\n",
    "--model {MODEL_HUB}  \\\n",
    "--tasks {TASK} \\\n",
    "--do_sample False \\\n",
    "--n_samples 1 \\\n",
    "--batch_size 1 \\\n",
    "--save_generations \\\n",
    "--trust_remote_code \\\n",
    "--prompt deepseek \\\n",
    "--save_generations_path {SAVE_PATH} \\\n",
    "--metric_output_path {EVAL_PATH} \\\n",
    "--max_length_generation 1024 \\\n",
    "--precision bf16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92ef71e4-3eaf-459e-81fa-3857f3d2c1d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-21T02:22:28.923611Z",
     "iopub.status.busy": "2024-08-21T02:22:28.922677Z",
     "iopub.status.idle": "2024-08-21T02:22:28.928981Z",
     "shell.execute_reply": "2024-08-21T02:22:28.928169Z",
     "shell.execute_reply.started": "2024-08-21T02:22:28.923497Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deepseekcoder6.7/generations_deepseek-coder-explain-6-7b-instruct\n",
      "deepseekcoder6.7/evaluation_compareeval_deepseek-coder-explain-6-7b-instruct\n"
     ]
    }
   ],
   "source": [
    "MODEL_HUB = \"deepseek-ai/deepseek-coder-6.7b-instruct\"\n",
    "MODEL = \"deepseek-coder-explain-6-7b-instruct\"\n",
    "PATH = \"deepseekcoder6.7\"\n",
    "TASK = \"compareeval\"\n",
    "QUANT = \"\"\n",
    "PEFT = \"Sam137/deepseek6.7-explain-coder\"\n",
    "\n",
    "def create_path(model, path, task, quant):\n",
    "    SAVE_PATH = path + \"/generations_\" + model + quant\n",
    "    LOAD_PATH = SAVE_PATH + \"_\" + task + \".json\"\n",
    "    EVAL_PATH = PATH + \"/evaluation_\" + TASK + \"_\" + model + quant\n",
    "    return SAVE_PATH, LOAD_PATH, EVAL_PATH\n",
    "\n",
    "SAVE_PATH, _, EVAL_PATH = create_path(MODEL, PATH, TASK, QUANT)\n",
    "print(SAVE_PATH)\n",
    "print(EVAL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14847a69-fe96-4c07-8447-9499a6ae5836",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-21T02:22:33.630447Z",
     "iopub.status.busy": "2024-08-21T02:22:33.630145Z",
     "iopub.status.idle": "2024-08-21T02:26:35.039700Z",
     "shell.execute_reply": "2024-08-21T02:26:35.039210Z",
     "shell.execute_reply.started": "2024-08-21T02:22:33.630425Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-21 02:22:36.475418: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-21 02:22:36.475491: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-21 02:22:36.476882: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-08-21 02:22:36.483928: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-21 02:22:37.441461: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Selected Tasks: ['compareeval']\n",
      "Loading model in bf16\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:02<00:00,  1.08s/it]\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/utils/hub.py:374: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "Loaded PEFT model. Merging...\n",
      "Merge complete.\n",
      "number of problems for this task is 100\n",
      "  0%|                                                   | 0/100 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:396: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:1281: UserWarning: Input length of input_ids is 1024, but `max_length` is set to 1024. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 100/100 [01:48<00:00,  1.08s/it]\n",
      "generations were saved at deepseekcoder6.7/generations_deepseek-coder-explain-6-7b-instruct_compareeval.json\n",
      "Evaluating generations...\n",
      "{\n",
      "  \"compareeval\": {\n",
      "    \"bleu\": 0.027515400410677623,\n",
      "    \"precisions\": [\n",
      "      0.02751540041067762\n",
      "    ],\n",
      "    \"brevity_penalty\": 1.0,\n",
      "    \"length_ratio\": 2.9865030674846627,\n",
      "    \"translation_length\": 2434,\n",
      "    \"reference_length\": 815\n",
      "  },\n",
      "  \"config\": {\n",
      "    \"prefix\": \"\",\n",
      "    \"do_sample\": false,\n",
      "    \"temperature\": 0.2,\n",
      "    \"top_k\": 0,\n",
      "    \"top_p\": 0.95,\n",
      "    \"n_samples\": 1,\n",
      "    \"eos\": \"<|endoftext|>\",\n",
      "    \"seed\": 0,\n",
      "    \"model\": \"deepseek-ai/deepseek-coder-6.7b-instruct\",\n",
      "    \"modeltype\": \"causal\",\n",
      "    \"peft_model\": \"Sam137/deepseek6.7-explain-coder\",\n",
      "    \"revision\": null,\n",
      "    \"use_auth_token\": false,\n",
      "    \"trust_remote_code\": true,\n",
      "    \"tasks\": \"compareeval\",\n",
      "    \"instruction_tokens\": null,\n",
      "    \"batch_size\": 1,\n",
      "    \"max_length_generation\": 1024,\n",
      "    \"precision\": \"bf16\",\n",
      "    \"load_in_8bit\": false,\n",
      "    \"load_in_4bit\": false,\n",
      "    \"left_padding\": false,\n",
      "    \"limit\": null,\n",
      "    \"limit_start\": 0,\n",
      "    \"save_every_k_tasks\": -1,\n",
      "    \"postprocess\": true,\n",
      "    \"allow_code_execution\": false,\n",
      "    \"generation_only\": false,\n",
      "    \"load_generations_path\": null,\n",
      "    \"load_data_path\": null,\n",
      "    \"metric_output_path\": \"deepseekcoder6.7/evaluation_compareeval_deepseek-coder-explain-6-7b-instruct\",\n",
      "    \"save_generations\": true,\n",
      "    \"load_generations_intermediate_paths\": null,\n",
      "    \"save_generations_path\": \"deepseekcoder6.7/generations_deepseek-coder-explain-6-7b-instruct\",\n",
      "    \"save_references\": false,\n",
      "    \"save_references_path\": \"references.json\",\n",
      "    \"prompt\": \"deepseek\",\n",
      "    \"max_memory_per_gpu\": null,\n",
      "    \"check_references\": false\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!python main.py \\\n",
    "--model {MODEL_HUB}  \\\n",
    "--peft_model {PEFT} \\\n",
    "--tasks {TASK} \\\n",
    "--do_sample False \\\n",
    "--n_samples 1 \\\n",
    "--batch_size 1 \\\n",
    "--save_generations \\\n",
    "--trust_remote_code \\\n",
    "--prompt deepseek \\\n",
    "--save_generations_path {SAVE_PATH} \\\n",
    "--metric_output_path {EVAL_PATH} \\\n",
    "--max_length_generation 1024 \\\n",
    "--precision bf16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "037067e1-a938-46a3-9584-97165c70bbcc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-21T02:17:05.509760Z",
     "iopub.status.busy": "2024-08-21T02:17:05.509569Z",
     "iopub.status.idle": "2024-08-21T02:17:05.513751Z",
     "shell.execute_reply": "2024-08-21T02:17:05.513339Z",
     "shell.execute_reply.started": "2024-08-21T02:17:05.509743Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deepseekcoder6.7/generations_deepseek-coder-compare-6-7b-instruct\n",
      "deepseekcoder6.7/evaluation_compareeval_deepseek-coder-compare-6-7b-instruct\n"
     ]
    }
   ],
   "source": [
    "MODEL_HUB = \"deepseek-ai/deepseek-coder-6.7b-instruct\"\n",
    "MODEL = \"deepseek-coder-compare-6-7b-instruct\"\n",
    "PATH = \"deepseekcoder6.7\"\n",
    "TASK = \"compareeval\"\n",
    "QUANT = \"\"\n",
    "PEFT = \"Sam137/deepseek6.7-compare-coder\"\n",
    "\n",
    "def create_path(model, path, task, quant):\n",
    "    SAVE_PATH = path + \"/generations_\" + model + quant\n",
    "    LOAD_PATH = SAVE_PATH + \"_\" + task + \".json\"\n",
    "    EVAL_PATH = PATH + \"/evaluation_\" + TASK + \"_\" + model + quant\n",
    "    return SAVE_PATH, LOAD_PATH, EVAL_PATH\n",
    "\n",
    "SAVE_PATH, _, EVAL_PATH = create_path(MODEL, PATH, TASK, QUANT)\n",
    "print(SAVE_PATH)\n",
    "print(EVAL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "891acaad-341a-428f-b1d9-47e857bc835f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-21T02:17:05.514832Z",
     "iopub.status.busy": "2024-08-21T02:17:05.514662Z",
     "iopub.status.idle": "2024-08-21T02:20:05.006862Z",
     "shell.execute_reply": "2024-08-21T02:20:05.006345Z",
     "shell.execute_reply.started": "2024-08-21T02:17:05.514817Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-21 02:17:08.318110: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-21 02:17:08.318192: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-21 02:17:08.319655: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-08-21 02:17:08.327236: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-21 02:17:09.216390: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Selected Tasks: ['compareeval']\n",
      "Loading model in bf16\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:03<00:00,  1.58s/it]\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/utils/hub.py:374: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "Loaded PEFT model. Merging...\n",
      "Merge complete.\n",
      "number of problems for this task is 100\n",
      "  0%|                                                   | 0/100 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:396: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:1281: UserWarning: Input length of input_ids is 1024, but `max_length` is set to 1024. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 100/100 [00:42<00:00,  2.36it/s]\n",
      "generations were saved at deepseekcoder6.7/generations_deepseek-coder-compare-6-7b-instruct_compareeval.json\n",
      "Evaluating generations...\n",
      "{\n",
      "  \"compareeval\": {\n",
      "    \"bleu\": 0.02040939001048708,\n",
      "    \"precisions\": [\n",
      "      0.3211009174311927\n",
      "    ],\n",
      "    \"brevity_penalty\": 0.06356067174694548,\n",
      "    \"length_ratio\": 0.26625766871165646,\n",
      "    \"translation_length\": 217,\n",
      "    \"reference_length\": 815\n",
      "  },\n",
      "  \"config\": {\n",
      "    \"prefix\": \"\",\n",
      "    \"do_sample\": false,\n",
      "    \"temperature\": 0.2,\n",
      "    \"top_k\": 0,\n",
      "    \"top_p\": 0.95,\n",
      "    \"n_samples\": 1,\n",
      "    \"eos\": \"<|endoftext|>\",\n",
      "    \"seed\": 0,\n",
      "    \"model\": \"deepseek-ai/deepseek-coder-6.7b-instruct\",\n",
      "    \"modeltype\": \"causal\",\n",
      "    \"peft_model\": \"Sam137/deepseek6.7-compare-coder\",\n",
      "    \"revision\": null,\n",
      "    \"use_auth_token\": false,\n",
      "    \"trust_remote_code\": true,\n",
      "    \"tasks\": \"compareeval\",\n",
      "    \"instruction_tokens\": null,\n",
      "    \"batch_size\": 1,\n",
      "    \"max_length_generation\": 1024,\n",
      "    \"precision\": \"bf16\",\n",
      "    \"load_in_8bit\": false,\n",
      "    \"load_in_4bit\": false,\n",
      "    \"left_padding\": false,\n",
      "    \"limit\": null,\n",
      "    \"limit_start\": 0,\n",
      "    \"save_every_k_tasks\": -1,\n",
      "    \"postprocess\": true,\n",
      "    \"allow_code_execution\": false,\n",
      "    \"generation_only\": false,\n",
      "    \"load_generations_path\": null,\n",
      "    \"load_data_path\": null,\n",
      "    \"metric_output_path\": \"deepseekcoder6.7/evaluation_compareeval_deepseek-coder-compare-6-7b-instruct\",\n",
      "    \"save_generations\": true,\n",
      "    \"load_generations_intermediate_paths\": null,\n",
      "    \"save_generations_path\": \"deepseekcoder6.7/generations_deepseek-coder-compare-6-7b-instruct\",\n",
      "    \"save_references\": false,\n",
      "    \"save_references_path\": \"references.json\",\n",
      "    \"prompt\": \"deepseek\",\n",
      "    \"max_memory_per_gpu\": null,\n",
      "    \"check_references\": false\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!python main.py \\\n",
    "--model {MODEL_HUB}  \\\n",
    "--peft_model {PEFT} \\\n",
    "--tasks {TASK} \\\n",
    "--do_sample False \\\n",
    "--n_samples 1 \\\n",
    "--batch_size 1 \\\n",
    "--save_generations \\\n",
    "--trust_remote_code \\\n",
    "--prompt deepseek \\\n",
    "--save_generations_path {SAVE_PATH} \\\n",
    "--metric_output_path {EVAL_PATH} \\\n",
    "--max_length_generation 1024 \\\n",
    "--precision bf16"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
