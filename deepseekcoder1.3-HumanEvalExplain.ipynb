{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dcf78c75-6c69-4c0d-906e-d0d5da7ed7fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T00:35:44.138731Z",
     "iopub.status.busy": "2024-08-11T00:35:44.138130Z",
     "iopub.status.idle": "2024-08-11T00:35:44.142328Z",
     "shell.execute_reply": "2024-08-11T00:35:44.141945Z",
     "shell.execute_reply.started": "2024-08-11T00:35:44.138710Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deepseekcoder6.7/generations_deepseek-coder-6-7b-instruct\n",
      "deepseekcoder6.7/generations_deepseek-coder-6-7b-instruct_humanevalexplaindescribe-python.json\n",
      "deepseekcoder6.7/evaluation_humanevalexplainpython_deepseek-coder-6-7b-instruct\n"
     ]
    }
   ],
   "source": [
    "MODEL_HUB = \"deepseek-ai/deepseek-coder-1.3b-instruct\"\n",
    "MODEL = \"deepseek-coder-1-3b-instruct\"\n",
    "PATH = \"deepseekcoder1.3\"\n",
    "TASK = \"humanevalexplaindescribe-python\"\n",
    "QUANT = \"\"\n",
    "\n",
    "def create_path(model, path, task, quant):\n",
    "    SAVE_PATH = path + \"/generations_\" + model + quant\n",
    "    LOAD_PATH = SAVE_PATH + \"_\" + task + \".json\"\n",
    "    EVAL_PATH = PATH + \"/evaluation_humanevalexplainpython_\" + model + quant\n",
    "    return SAVE_PATH, LOAD_PATH, EVAL_PATH\n",
    "\n",
    "SAVE_PATH, LOAD_PATH, EVAL_PATH = create_path(MODEL, PATH, TASK, QUANT)\n",
    "print(SAVE_PATH)\n",
    "print(LOAD_PATH)\n",
    "print(EVAL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4afc6e7-49fd-49f2-95b3-65289342ea4f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-10T23:29:02.289966Z",
     "iopub.status.busy": "2024-08-10T23:29:02.289824Z",
     "iopub.status.idle": "2024-08-10T23:41:39.487147Z",
     "shell.execute_reply": "2024-08-10T23:41:39.486656Z",
     "shell.execute_reply.started": "2024-08-10T23:29:02.289953Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-10 23:29:04.834054: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-10 23:29:04.834119: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-10 23:29:04.835181: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-08-10 23:29:04.840902: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-10 23:29:05.651159: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Selected Tasks: ['humanevalexplaindescribe-python']\n",
      "Loading model in fp16\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:02<00:00,  1.08s/it]\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/utils/hub.py:374: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "generation mode only\n",
      "number of problems for this task is 164\n",
      "  0%|                                                   | 0/164 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:396: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 164/164 [10:13<00:00,  3.74s/it]\n",
      "generations were saved at deepseekcoder6.7/generations_deepseek-coder-6-7b-instruct_humanevalexplaindescribe-python.json\n"
     ]
    }
   ],
   "source": [
    "!python main.py \\\n",
    "--model {MODEL_HUB}  \\\n",
    "--tasks {TASK} \\\n",
    "--generation_only \\\n",
    "--do_sample False \\\n",
    "--n_samples 1 \\\n",
    "--batch_size 1 \\\n",
    "--allow_code_execution \\\n",
    "--save_generations \\\n",
    "--trust_remote_code \\\n",
    "--prompt deepseek \\\n",
    "--save_generations_path \"{SAVE_PATH}\" \\\n",
    "--max_length_generation 2048 \\\n",
    "--precision fp16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dfee00fc-8a08-4a5f-8821-b8934c2ac859",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T00:35:50.027183Z",
     "iopub.status.busy": "2024-08-11T00:35:50.026656Z",
     "iopub.status.idle": "2024-08-11T00:35:50.029254Z",
     "shell.execute_reply": "2024-08-11T00:35:50.028935Z",
     "shell.execute_reply.started": "2024-08-11T00:35:50.027165Z"
    }
   },
   "outputs": [],
   "source": [
    "TASK = \"humanevalexplainsynthesize-python\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4bf5ace8-4358-4c1c-a68f-0ab872011cc4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T00:36:11.245200Z",
     "iopub.status.busy": "2024-08-11T00:36:11.244529Z",
     "iopub.status.idle": "2024-08-11T00:47:03.007505Z",
     "shell.execute_reply": "2024-08-11T00:47:03.006946Z",
     "shell.execute_reply.started": "2024-08-11T00:36:11.245177Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-11 00:36:13.660317: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-11 00:36:13.660372: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-11 00:36:13.661362: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-08-11 00:36:13.666887: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-11 00:36:14.496362: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Selected Tasks: ['humanevalexplainsynthesize-python']\n",
      "Loading model in fp16\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:02<00:00,  1.22s/it]\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/utils/hub.py:374: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "164 descriptions with 1 description candidates loaded.\n",
      "164 descriptions with 1 description candidates loaded.\n",
      "number of problems for this task is 164\n",
      "  0%|                                                   | 0/164 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:396: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 164/164 [08:01<00:00,  2.94s/it]\n",
      "generations were saved at deepseekcoder6.7/generations_deepseek-coder-6-7b-instruct_humanevalexplainsynthesize-python.json\n",
      "Evaluating generations...\n",
      "{\n",
      "  \"humanevalexplainsynthesize-python\": {\n",
      "    \"pass@1\": 0.6280487804878049\n",
      "  },\n",
      "  \"config\": {\n",
      "    \"prefix\": \"\",\n",
      "    \"do_sample\": false,\n",
      "    \"temperature\": 0.2,\n",
      "    \"top_k\": 0,\n",
      "    \"top_p\": 0.95,\n",
      "    \"n_samples\": 1,\n",
      "    \"eos\": \"<|endoftext|>\",\n",
      "    \"seed\": 0,\n",
      "    \"model\": \"deepseek-ai/deepseek-coder-6.7b-instruct\",\n",
      "    \"modeltype\": \"causal\",\n",
      "    \"peft_model\": null,\n",
      "    \"revision\": null,\n",
      "    \"use_auth_token\": false,\n",
      "    \"trust_remote_code\": true,\n",
      "    \"tasks\": \"humanevalexplainsynthesize-python\",\n",
      "    \"instruction_tokens\": null,\n",
      "    \"batch_size\": 1,\n",
      "    \"max_length_generation\": 2048,\n",
      "    \"precision\": \"fp16\",\n",
      "    \"load_in_8bit\": false,\n",
      "    \"load_in_4bit\": false,\n",
      "    \"left_padding\": false,\n",
      "    \"limit\": null,\n",
      "    \"limit_start\": 0,\n",
      "    \"save_every_k_tasks\": -1,\n",
      "    \"postprocess\": true,\n",
      "    \"allow_code_execution\": true,\n",
      "    \"generation_only\": false,\n",
      "    \"load_generations_path\": null,\n",
      "    \"load_data_path\": \"deepseekcoder6.7/generations_deepseek-coder-6-7b-instruct_humanevalexplaindescribe-python.json\",\n",
      "    \"metric_output_path\": \"deepseekcoder6.7/evaluation_humanevalexplainpython_deepseek-coder-6-7b-instruct\",\n",
      "    \"save_generations\": true,\n",
      "    \"load_generations_intermediate_paths\": null,\n",
      "    \"save_generations_path\": \"deepseekcoder6.7/generations_deepseek-coder-6-7b-instruct\",\n",
      "    \"save_references\": false,\n",
      "    \"save_references_path\": \"references.json\",\n",
      "    \"prompt\": \"deepseek\",\n",
      "    \"max_memory_per_gpu\": null,\n",
      "    \"check_references\": false\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!python main.py \\\n",
    "--model {MODEL_HUB}  \\\n",
    "--tasks {TASK} \\\n",
    "--do_sample False \\\n",
    "--n_samples 1 \\\n",
    "--batch_size 1 \\\n",
    "--allow_code_execution \\\n",
    "--save_generations \\\n",
    "--trust_remote_code \\\n",
    "--prompt deepseek \\\n",
    "--load_data_path {LOAD_PATH} \\\n",
    "--save_generations_path {SAVE_PATH} \\\n",
    "--metric_output_path {EVAL_PATH} \\\n",
    "--max_length_generation 2048 \\\n",
    "--precision fp16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fa76d3c8-465d-4ba5-9e1a-8cc0ee6fc69e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T00:50:20.024476Z",
     "iopub.status.busy": "2024-08-11T00:50:20.023810Z",
     "iopub.status.idle": "2024-08-11T00:50:20.028550Z",
     "shell.execute_reply": "2024-08-11T00:50:20.028039Z",
     "shell.execute_reply.started": "2024-08-11T00:50:20.024452Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deepseekcoder6.7/generations_deepseek-coder-6-7b-instruct8bit\n",
      "deepseekcoder6.7/generations_deepseek-coder-6-7b-instruct8bit_humanevalexplaindescribe-python.json\n",
      "deepseekcoder6.7/evaluation_humanevalexplainpython_deepseek-coder-6-7b-instruct8bit\n"
     ]
    }
   ],
   "source": [
    "MODEL_HUB = \"deepseek-ai/deepseek-coder-1.3b-instruct\"\n",
    "MODEL = \"deepseek-coder-1-3b-instruct\"\n",
    "PATH = \"deepseekcoder1.3\"\n",
    "TASK = \"humanevalexplaindescribe-python\"\n",
    "QUANT = \"8bit\"\n",
    "\n",
    "def create_path(model, path, task, quant):\n",
    "    SAVE_PATH = path + \"/generations_\" + model + quant\n",
    "    LOAD_PATH = SAVE_PATH + \"_\" + task + \".json\"\n",
    "    EVAL_PATH = PATH + \"/evaluation_humanevalexplainpython_\" + model + quant\n",
    "    return SAVE_PATH, LOAD_PATH, EVAL_PATH\n",
    "\n",
    "SAVE_PATH, LOAD_PATH, EVAL_PATH = create_path(MODEL, PATH, TASK, QUANT)\n",
    "print(SAVE_PATH)\n",
    "print(LOAD_PATH)\n",
    "print(EVAL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5c8761b-1dca-42f3-a9b0-79936754825b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-10T23:41:44.989052Z",
     "iopub.status.busy": "2024-08-10T23:41:44.988913Z",
     "iopub.status.idle": "2024-08-11T00:13:58.021829Z",
     "shell.execute_reply": "2024-08-11T00:13:58.021374Z",
     "shell.execute_reply.started": "2024-08-10T23:41:44.989040Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-10 23:41:47.409290: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-10 23:41:47.409350: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-10 23:41:47.410377: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-08-10 23:41:47.415910: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-10 23:41:48.238854: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Selected Tasks: ['humanevalexplaindescribe-python']\n",
      "Loading model in 8bit\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:06<00:00,  3.08s/it]\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/utils/hub.py:374: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "generation mode only\n",
      "number of problems for this task is 164\n",
      "  0%|                                                   | 0/164 [00:00<?, ?it/s][2024-08-10 23:42:00,111] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:396: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 164/164 [31:56<00:00, 11.69s/it]\n",
      "generations were saved at deepseekcoder6.7/generations_deepseek-coder-6-7b-instruct8bit_humanevalexplaindescribe-python.json\n"
     ]
    }
   ],
   "source": [
    "!python main.py \\\n",
    "--model {MODEL_HUB}  \\\n",
    "--tasks humanevalexplaindescribe-python \\\n",
    "--generation_only \\\n",
    "--do_sample False \\\n",
    "--n_samples 1 \\\n",
    "--batch_size 1 \\\n",
    "--allow_code_execution \\\n",
    "--save_generations \\\n",
    "--trust_remote_code \\\n",
    "--prompt deepseek \\\n",
    "--save_generations_path \"{SAVE_PATH}\" \\\n",
    "--max_length_generation 2048 \\\n",
    "--precision fp16 \\\n",
    "--load_in_8bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ce9378a6-80cc-4976-9b0a-dda3e79afaba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T00:50:24.185193Z",
     "iopub.status.busy": "2024-08-11T00:50:24.184985Z",
     "iopub.status.idle": "2024-08-11T00:50:24.187772Z",
     "shell.execute_reply": "2024-08-11T00:50:24.187345Z",
     "shell.execute_reply.started": "2024-08-11T00:50:24.185177Z"
    }
   },
   "outputs": [],
   "source": [
    "TASK = \"humanevalexplainsynthesize-python\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "72f5cc02-d4e5-4c15-989f-ab49990296dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T00:50:28.753993Z",
     "iopub.status.busy": "2024-08-11T00:50:28.753427Z",
     "iopub.status.idle": "2024-08-11T01:13:59.382660Z",
     "shell.execute_reply": "2024-08-11T01:13:59.382112Z",
     "shell.execute_reply.started": "2024-08-11T00:50:28.753973Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-11 00:50:31.257910: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-11 00:50:31.257969: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-11 00:50:31.258994: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-08-11 00:50:31.264499: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-11 00:50:32.082464: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Selected Tasks: ['humanevalexplainsynthesize-python']\n",
      "Loading model in 8bit\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:06<00:00,  3.08s/it]\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/utils/hub.py:374: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "164 descriptions with 1 description candidates loaded.\n",
      "164 descriptions with 1 description candidates loaded.\n",
      "number of problems for this task is 164\n",
      "  0%|                                                   | 0/164 [00:00<?, ?it/s][2024-08-11 00:50:43,690] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:396: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 164/164 [22:46<00:00,  8.33s/it]\n",
      "generations were saved at deepseekcoder6.7/generations_deepseek-coder-6-7b-instruct8bit_humanevalexplainsynthesize-python.json\n",
      "Evaluating generations...\n",
      "{\n",
      "  \"humanevalexplainsynthesize-python\": {\n",
      "    \"pass@1\": 0.5975609756097561\n",
      "  },\n",
      "  \"config\": {\n",
      "    \"prefix\": \"\",\n",
      "    \"do_sample\": false,\n",
      "    \"temperature\": 0.2,\n",
      "    \"top_k\": 0,\n",
      "    \"top_p\": 0.95,\n",
      "    \"n_samples\": 1,\n",
      "    \"eos\": \"<|endoftext|>\",\n",
      "    \"seed\": 0,\n",
      "    \"model\": \"deepseek-ai/deepseek-coder-6.7b-instruct\",\n",
      "    \"modeltype\": \"causal\",\n",
      "    \"peft_model\": null,\n",
      "    \"revision\": null,\n",
      "    \"use_auth_token\": false,\n",
      "    \"trust_remote_code\": true,\n",
      "    \"tasks\": \"humanevalexplainsynthesize-python\",\n",
      "    \"instruction_tokens\": null,\n",
      "    \"batch_size\": 1,\n",
      "    \"max_length_generation\": 2048,\n",
      "    \"precision\": \"fp16\",\n",
      "    \"load_in_8bit\": true,\n",
      "    \"load_in_4bit\": false,\n",
      "    \"left_padding\": false,\n",
      "    \"limit\": null,\n",
      "    \"limit_start\": 0,\n",
      "    \"save_every_k_tasks\": -1,\n",
      "    \"postprocess\": true,\n",
      "    \"allow_code_execution\": true,\n",
      "    \"generation_only\": false,\n",
      "    \"load_generations_path\": null,\n",
      "    \"load_data_path\": \"deepseekcoder6.7/generations_deepseek-coder-6-7b-instruct8bit_humanevalexplaindescribe-python.json\",\n",
      "    \"metric_output_path\": \"deepseekcoder6.7/evaluation_humanevalexplainpython_deepseek-coder-6-7b-instruct8bit\",\n",
      "    \"save_generations\": true,\n",
      "    \"load_generations_intermediate_paths\": null,\n",
      "    \"save_generations_path\": \"deepseekcoder6.7/generations_deepseek-coder-6-7b-instruct8bit\",\n",
      "    \"save_references\": false,\n",
      "    \"save_references_path\": \"references.json\",\n",
      "    \"prompt\": \"deepseek\",\n",
      "    \"max_memory_per_gpu\": null,\n",
      "    \"check_references\": false\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!python main.py \\\n",
    "--model {MODEL_HUB}  \\\n",
    "--tasks {TASK} \\\n",
    "--do_sample False \\\n",
    "--n_samples 1 \\\n",
    "--batch_size 1 \\\n",
    "--allow_code_execution \\\n",
    "--save_generations \\\n",
    "--trust_remote_code \\\n",
    "--prompt deepseek \\\n",
    "--load_data_path {LOAD_PATH} \\\n",
    "--save_generations_path {SAVE_PATH} \\\n",
    "--metric_output_path {EVAL_PATH} \\\n",
    "--max_length_generation 2048 \\\n",
    "--precision fp16 \\\n",
    "--load_in_8bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f54816b-1900-4dce-a23c-d94c5a78f214",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T00:14:03.660499Z",
     "iopub.status.busy": "2024-08-11T00:14:03.660335Z",
     "iopub.status.idle": "2024-08-11T00:14:03.664703Z",
     "shell.execute_reply": "2024-08-11T00:14:03.664257Z",
     "shell.execute_reply.started": "2024-08-11T00:14:03.660480Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deepseekcoder6.7/generations_deepseek-coder-6-7b-instruct4bit\n",
      "deepseekcoder6.7/generations_deepseek-coder-6-7b-instruct4bit_humanevalexplaindescribe-python.json\n",
      "deepseekcoder6.7/evaluation_humanevalexplainpython_deepseek-coder-6-7b-instruct4bit\n"
     ]
    }
   ],
   "source": [
    "MODEL_HUB = \"deepseek-ai/deepseek-coder-1.3b-instruct\"\n",
    "MODEL = \"deepseek-coder-1-3b-instruct\"\n",
    "PATH = \"deepseekcoder1.3\"\n",
    "TASK = \"humanevalexplaindescribe-python\"\n",
    "QUANT = \"4bit\"\n",
    "\n",
    "def create_path(model, path, task, quant):\n",
    "    SAVE_PATH = path + \"/generations_\" + model + quant\n",
    "    LOAD_PATH = SAVE_PATH + \"_\" + task + \".json\"\n",
    "    EVAL_PATH = PATH + \"/evaluation_humanevalexplainpython_\" + model + quant\n",
    "    return SAVE_PATH, LOAD_PATH, EVAL_PATH\n",
    "\n",
    "SAVE_PATH, LOAD_PATH, EVAL_PATH = create_path(MODEL, PATH, TASK, QUANT)\n",
    "print(SAVE_PATH)\n",
    "print(LOAD_PATH)\n",
    "print(EVAL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2673d1df-5fe9-4ece-8ce8-8b8601b50fc8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T00:14:03.665303Z",
     "iopub.status.busy": "2024-08-11T00:14:03.665172Z",
     "iopub.status.idle": "2024-08-11T00:22:58.620967Z",
     "shell.execute_reply": "2024-08-11T00:22:58.620431Z",
     "shell.execute_reply.started": "2024-08-11T00:14:03.665290Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-11 00:14:06.081196: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-11 00:14:06.081254: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-11 00:14:06.082245: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-08-11 00:14:06.087771: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-11 00:14:06.900498: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Selected Tasks: ['humanevalexplaindescribe-python']\n",
      "Loading model in 4bit\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:06<00:00,  3.17s/it]\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/utils/hub.py:374: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "generation mode only\n",
      "number of problems for this task is 164\n",
      "  0%|                                                   | 0/164 [00:00<?, ?it/s][2024-08-11 00:14:20,058] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:396: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 164/164 [08:37<00:00,  3.15s/it]\n",
      "generations were saved at deepseekcoder6.7/generations_deepseek-coder-6-7b-instruct4bit_humanevalexplaindescribe-python.json\n"
     ]
    }
   ],
   "source": [
    "!python main.py \\\n",
    "--model {MODEL_HUB}  \\\n",
    "--tasks humanevalexplaindescribe-python \\\n",
    "--generation_only \\\n",
    "--do_sample False \\\n",
    "--n_samples 1 \\\n",
    "--batch_size 1 \\\n",
    "--allow_code_execution \\\n",
    "--save_generations \\\n",
    "--trust_remote_code \\\n",
    "--prompt deepseek \\\n",
    "--save_generations_path \"{SAVE_PATH}\" \\\n",
    "--max_length_generation 2048 \\\n",
    "--precision fp16 \\\n",
    "--load_in_4bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9433b623-68ba-4d8e-b1ae-b07b2da2de92",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T00:24:39.989682Z",
     "iopub.status.busy": "2024-08-11T00:24:39.989457Z",
     "iopub.status.idle": "2024-08-11T00:24:39.993090Z",
     "shell.execute_reply": "2024-08-11T00:24:39.992567Z",
     "shell.execute_reply.started": "2024-08-11T00:24:39.989664Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deepseekcoder6.7/generations_deepseek-coder-6-7b-instruct4bit_humanevalexplaindescribe-python.json\n"
     ]
    }
   ],
   "source": [
    "TASK = \"humanevalexplainsynthesize-python\"\n",
    "print(LOAD_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "886a584f-f513-48dd-8697-3a411b2fc754",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T00:26:38.288966Z",
     "iopub.status.busy": "2024-08-11T00:26:38.288356Z",
     "iopub.status.idle": "2024-08-11T00:34:21.803233Z",
     "shell.execute_reply": "2024-08-11T00:34:21.802707Z",
     "shell.execute_reply.started": "2024-08-11T00:26:38.288947Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-11 00:26:40.768168: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-11 00:26:40.768231: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-11 00:26:40.769268: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-08-11 00:26:40.775025: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-11 00:26:41.588061: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Selected Tasks: ['humanevalexplainsynthesize-python']\n",
      "Loading model in 4bit\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:06<00:00,  3.37s/it]\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/utils/hub.py:374: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "164 descriptions with 1 description candidates loaded.\n",
      "164 descriptions with 1 description candidates loaded.\n",
      "number of problems for this task is 164\n",
      "  0%|                                                   | 0/164 [00:00<?, ?it/s][2024-08-11 00:26:53,710] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:396: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 164/164 [06:59<00:00,  2.56s/it]\n",
      "generations were saved at deepseekcoder6.7/generations_deepseek-coder-6-7b-instruct4bit_humanevalexplainsynthesize-python.json\n",
      "Evaluating generations...\n",
      "{\n",
      "  \"humanevalexplainsynthesize-python\": {\n",
      "    \"pass@1\": 0.5670731707317073\n",
      "  },\n",
      "  \"config\": {\n",
      "    \"prefix\": \"\",\n",
      "    \"do_sample\": false,\n",
      "    \"temperature\": 0.2,\n",
      "    \"top_k\": 0,\n",
      "    \"top_p\": 0.95,\n",
      "    \"n_samples\": 1,\n",
      "    \"eos\": \"<|endoftext|>\",\n",
      "    \"seed\": 0,\n",
      "    \"model\": \"deepseek-ai/deepseek-coder-6.7b-instruct\",\n",
      "    \"modeltype\": \"causal\",\n",
      "    \"peft_model\": null,\n",
      "    \"revision\": null,\n",
      "    \"use_auth_token\": false,\n",
      "    \"trust_remote_code\": true,\n",
      "    \"tasks\": \"humanevalexplainsynthesize-python\",\n",
      "    \"instruction_tokens\": null,\n",
      "    \"batch_size\": 1,\n",
      "    \"max_length_generation\": 2048,\n",
      "    \"precision\": \"fp16\",\n",
      "    \"load_in_8bit\": false,\n",
      "    \"load_in_4bit\": true,\n",
      "    \"left_padding\": false,\n",
      "    \"limit\": null,\n",
      "    \"limit_start\": 0,\n",
      "    \"save_every_k_tasks\": -1,\n",
      "    \"postprocess\": true,\n",
      "    \"allow_code_execution\": true,\n",
      "    \"generation_only\": false,\n",
      "    \"load_generations_path\": null,\n",
      "    \"load_data_path\": \"deepseekcoder6.7/generations_deepseek-coder-6-7b-instruct4bit_humanevalexplaindescribe-python.json\",\n",
      "    \"metric_output_path\": \"deepseekcoder6.7/evaluation_humanevalexplainpython_deepseek-coder-6-7b-instruct4bit\",\n",
      "    \"save_generations\": true,\n",
      "    \"load_generations_intermediate_paths\": null,\n",
      "    \"save_generations_path\": \"deepseekcoder6.7/generations_deepseek-coder-6-7b-instruct4bit\",\n",
      "    \"save_references\": false,\n",
      "    \"save_references_path\": \"references.json\",\n",
      "    \"prompt\": \"deepseek\",\n",
      "    \"max_memory_per_gpu\": null,\n",
      "    \"check_references\": false\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!python main.py \\\n",
    "--model {MODEL_HUB}  \\\n",
    "--tasks {TASK} \\\n",
    "--do_sample False \\\n",
    "--n_samples 1 \\\n",
    "--batch_size 1 \\\n",
    "--allow_code_execution \\\n",
    "--save_generations \\\n",
    "--trust_remote_code \\\n",
    "--prompt deepseek \\\n",
    "--load_data_path {LOAD_PATH} \\\n",
    "--save_generations_path {SAVE_PATH} \\\n",
    "--metric_output_path {EVAL_PATH} \\\n",
    "--max_length_generation 2048 \\\n",
    "--precision fp16 \\\n",
    "--load_in_4bit"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
